\vspace{12pt}
\documentclass[12pt]{article}
% version = 1.00 of latexdemo.tex 2011 Feb 01

% Atul Singh Arora
% BS-MS 2016 | IISER Mohali
% Winters, 2012
% Physics Project

%I did not add this
\usepackage{html}
%for making the real number R symbol to work
\usepackage{amsfonts}
%for getting a pipe symbol to work
\usepackage[T1]{fontenc}
%for increasing usable page area
\usepackage[margin=0.5in]{geometry}
%for automatically skipping a line after each paragraph
\usepackage[parfill]{parskip}
%for using text within formulae
\usepackage{amstext}
%for inserting images
\usepackage{graphicx}

%for proper enumeration
\usepackage{enumerate}
%for math theorem stuff!
\usepackage{amsmath, amsthm, amssymb}
%for cancelling things!
\usepackage{cancel}	

%for a double sided arrow
\usepackage{mathtools}
\rmfamily
%for changing the way the title in the section appears
\usepackage{sectsty}
%this is to make the rest of the section titles go SC
\allsectionsfont{\mdseries\scshape}
%This is to make the main sections' titles go SC but smaller than default
\sectionfont{\mdseries\scshape\fontsize{13.5}{1}\selectfont}
\subsectionfont{\mdseries\scshape\fontsize{13}{1}\selectfont}
\subsubsectionfont{\mdseries\scshape\fontsize{12}{1}\selectfont}

%for using system fonts
% \usepackage[xetex]{graphicx}
% \usepackage{fontspec,xunicode}
% \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
% \setmainfont[Scale=.95]{Times}
% \setmonofont{Lucida Sans Typewriter}
% \usepackage{Antiqua}

%for making things bold easily, as we're working with a lot of vectors here
\def\*#1{\boldsymbol{#1}}
%for making unit vectors
\let\oldhat\hat
\renewcommand{\hat}[1]{\oldhat{\*{#1}}}

%for writing dual correspondance easily
\def\dc{\,\,\xleftrightarrow[\text{DC}]{}\,\,}
%for writing bras and kets
\def\bra#1{\langle#1|}
\def\ket#1{|#1 \rangle}
%inner product
\def\inpr#1#2{\langle #1|#2 \rangle}
%outer product
\def\oupr#1#2{| #1 \rangle \langle #2 |}
%bracket
\def\braket#1#2#3{\langle#1|#2|#3\rangle}
%hermitian adjoint
\def\ha#1{#1^\dagger}
%expectation value
\def\expval#1{\langle #1 \rangle}

%Theorems one can use
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}

\begin{document}

\bibliographystyle{unsrt}  % define bibliography style



\begin{center}

% \title{Quantum Mechanics}
% \author{Atul Singh Arora}
% \maketitle

\textsc{{\huge Quantum Mechanics\\}
Introduction\\
\small WP Status\\}
\begin{minipage}{0.4\textwidth}
\begin{flushleft} Atul Singh Arora \end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} {\small December 9-14, 2012} \end{flushright}
\end{minipage}
\\
\end{center}
\hrule
\vspace{12pt}

This document contains record of my understanding of Chapter 1 Fundamental Concepts, from J.J. Sakurai.
\par
Areas marked with a {\bf Doubt} or {\bf Find out} are ones I am not absolutely clear about. Perhaps reiterating later would help.\\
\hrule
\section{Introduction}
	\subsection{Stern Gerlach}
		Silver atom has $47$ electrons, of which $46$ are paired. The $47^{th}$ has a spin, but no orbital-angular momentum (because it's an s electron). {\bf Doubt} In accordance with the book, and I quote `The $47$ electrons are attached to the nucleus, which is $~2\times 10^5$ times heavier than the electron; as a result, the heavy atom as a whole possesses a magnetic moment equal to the spin magnetic moment of the $47^{th}$ electron'. Does that mean that if the electrons were comparable in mass with the nucleus, we would couldn't have claimed that the magnetic moment of the atom is same as that of the electron? I can not see the co-relation between spin angular moment and mass. Also, why is the nuclear spin ignored and how is it justified?
		\par
		Force on a magnetic dipole is given by $\*F=\nabla(\*\mu. \*B)$. For a Magnetic field varying along z only, we therefore have
		\begin{equation}
			F_z = \mu_z \frac{\partial{B_z}}{\partial{z}}
		\end{equation}
		So we would expect the Stern Gerlach experiment to split the two beams into two, along the $z$ axis. And this is what was observed. However things get interesting when we cascade these experiments. The beam from the $\hat z +$ from the SG $\hat z$ apparatus is allowed to go through the SG $\hat z$ again, and we observe only the S $\hat z +$ component. Which is again as expected. However, if the $\hat z +$ beam is passed through an SG $\hat x$ apparatus, we get both $\hat x + $ and $\hat x -$ beams. Now also, one may be able to rationalize the result by saying the incident beam had both $\hat z \pm$ and $\hat x \pm$ and after blocking $\hat z -$, we had $\hat z +$ and $\hat x \pm$. Then on splitting by SG $\hat x$, we got $\hat x \pm$. However, this gets into trouble with the final blow. Here's the setup. SG $\hat z$, blocked $\hat z -$, SG $\hat x$, blocked $\hat x -$, SG $\hat z$, and we get $\hat x \pm$!
		\par
		The book at this stage, points out the following
		\begin{itemize}
			\item Spin components along $\hat z$ and $\hat y$ can't be measured simultaneously
			\item This more \emph{precisely} means that selection of the $\hat x$ component by the SG $\hat x$ apparatus, removes any information about the $\hat z$ component.
		\end{itemize}
		The book then gives an analogy with the book, which by itself is substantially clear, though long and has been omitted from the discussion.
		\subsection{Kets Bras and Operators}
			\subsubsection{Ket Space}
				State of a system is represented by a \emph{state vector}, which is known as \emph{ket}, and is denoted by $|\alpha>$. The state vector is postulated to contains all information retrievable about the system. Following are properties of kets, arbitrarily defined to be true at this stage
				\begin{enumerate}
					\item $|\alpha> + |\beta> = |\gamma>$
					\item $c|\alpha> = |\alpha> c$, where $c$ is complex and if $c=0$, the resultant is a \emph{null ket}.
					\item \emph{Observables} are represented by \emph{operators} which act on kets as $A.(|\alpha>) = A|\alpha>$
						\begin{enumerate}
							\item in general, $A|\alpha>$ is not of the type $c|\alpha>$ (where c is complex)
							\item for \emph{eigenkets} of $A$, the operation is always a scalar multiple of the eigenket and the scalar is called the \emph{eigenvalue}
							\item nomenclature: it is typical to represent eigenkets with eigenvalues ${a',a'',a''',...}$ by ${|a'>,|a''>,|a'''>,...}$ respectively.
							\item familiar rules related to vector spaces: An N-dimensional vector space is spanned by the N eigenkets of the observable A.
						\end{enumerate}
				\end{enumerate}
			\subsubsection{Bra Space and Inner Products}
				Bra space is `dual to' the ket space. Why this must be introduced is a mystery as of now, but it should become clear soon enough.
				\begin{enumerate}
					\item Postulate: For every ket, $|\alpha>$ in the ket space, $\exists$ a bra $<\alpha|$ in the bra space.
						\begin{enumerate}
							\item $|\alpha> \dc <\alpha|$
							\item $|\alpha> + |\beta> \dc <\alpha| + <\beta|$
							\item $c_\alpha|\alpha> + c_\beta|\beta> \dc c_\alpha^*<\alpha| + c_\beta^*<\beta|$
						\end{enumerate}
					\item The bra space is spanned by \emph{eigenbras}, the bras dual to the eigenkets			
					\item \emph{Inner product} is defined as $\inpr{\beta}{\alpha} = (\bra\beta)(\ket\alpha)$, bra(c)ket! The answer is a complex number.
						\begin{enumerate}
							\item Postulate: $\inpr{\beta}{\alpha}=\inpr{\alpha}{\beta}^*$ \\
							Easy deduction: $\Rightarrow \inpr{\alpha}{\alpha}$ is a real number.
							\item Postulate of \emph{Positive Definite Metric}: $\inpr{\alpha}{\alpha} \ge 0$, where equality holds iff $\ket \alpha$ is a null ket.
						\end{enumerate}
					\item Iff $\inpr{\alpha}{\beta}=0$, then $\ket \alpha$ and $\ket \beta$ are \emph{orthogonal}.
					\item \emph{Norm} of a ket $\ket \alpha$ is given by $\sqrt{\inpr{\alpha}{\alpha}}$
					\item A \emph{Normalized ket} is given by 
					\begin{equation*}
						\ket{\tilde{\alpha}} = \frac{1}{\sqrt{\inpr{\alpha}{\alpha}}} \ket{\alpha}
					\end{equation*}
						so that for a normalized ket, we get $\inpr{\tilde{\alpha}}{\tilde{\alpha}} = 1$
				\end{enumerate}
			\subsubsection{Operators}				
				\begin{enumerate}
					\item An operator (represented by X, Y etc.) act on kets from the left to result in another ket. They act on bras from the right.
					\begin{enumerate}
						\item Two operators X and Y are equal iff $\forall \; \; \ket \alpha$, we have $X\ket \alpha = Y\ket \alpha$
						\item X is a \emph{null operator} iff $X\ket \alpha = 0 \; \forall \;\; \ket \alpha$
					\end{enumerate}
					\item Addition of Operators
					\begin{enumerate}
						\item For operators, addition operations are commutative and associative
						\item Operators are also linear, viz. $X(c_\alpha \ket\alpha + c_\beta \ket \beta) = c_\alpha X \ket \alpha + c_\beta X \ket \beta$, except for an exception of the time-reversal operator
					\end{enumerate}
					\item Relations between Operation on Bras and Kets
					\begin{enumerate}
						\item $X \ket \alpha \cancel \dc \bra \alpha X$ in general
						\item The \emph{Hermitian adjoint}, $X^\dagger$ is defined as $X \ket \alpha \dc \bra \alpha X^\dagger$
						\item An operator is \emph{Hermitian} iff $X=X^\dagger$
					\end{enumerate}
					\item Multiplication of Operators
						\begin{enumerate}
							\item Multiplication of operators is not commutative, but it is associative\\
							Associativity holds good for all legal multiplications, viz. the ones defined here
							\item $X(Y \ket \alpha) = (XY) \ket \alpha = XY \ket \alpha$ and similarly $(\bra \beta X)Y = \bra \beta (XY) = \bra \beta XY$
							\item $\ha{(XY)}=\ha Y \ha X$
							\begin{proof}
							\begin{multline*}
								\text{We know}\;
								 		(XY)\ket \alpha \dc \bra \alpha \ha{(XY)} \\
								\shoveleft{\text{We also know}\;
								 	(Y\ket \alpha) \dc (\bra \alpha \ha Y)} \\
								\shoveleft{\text{Let}\;
												Y\ket\alpha = \ket \beta } \\
								\shoveleft{\Rightarrow			Y\ket\alpha = \ket \beta \dc \bra \beta = \bra \alpha \ha Y} \\
								\shoveleft{\text{Then}\;
											X\ket \beta \dc \bra \beta \ha X }\\
								\shoveleft{\Rightarrow			XY \ket \alpha \dc \bra \alpha \ha Y \ha X}\\
							\end{multline*}
							\end{proof}
						\end{enumerate}
					\item \emph{Outer Product} is defined as $(\ket \beta) (\bra \alpha) = \ket \beta \bra \alpha$.
						\begin{enumerate}
							\item This is not a number, it's an operator.
							\begin{proof}
							Consider $(\ket \beta \bra \alpha) \ket \gamma $ which by associativity, we can write as\\
							 $\ket \beta (\bra \alpha \ket \gamma) = \ket \beta (\inpr{\alpha}{\gamma})$
							\end{proof}
							\item If $X=\ket \beta \bra \alpha$, then $\ha X = \ket \alpha \bra \beta$
							\begin{proof}
								$X=\ket \beta \bra \alpha$, so\\
								$X\ket \gamma = (\ket \beta \bra \alpha).\ket \gamma = \ket \beta (\inpr{\alpha}{\gamma}) \dc \bra \beta (\inpr{\alpha}{\gamma})^* = (\inpr{\gamma}{\alpha})\bra \beta = \bra \gamma (\ket \alpha \bra \beta) = \bra \gamma \ha X$
							\end{proof}

						\end{enumerate}
					\item Since $(\bra \beta ) (X \ket \alpha ) = (\bra \beta X ) (\ket \alpha)$, we denote it by a simpler notation $\braket \beta X \alpha$.\\
					Now we claim 
					\begin{equation}
						\braket \beta X \alpha = \braket \alpha {\ha X} \beta ^*
						\label{hermitian_adjoint_operator}
					\end{equation}
					\begin{proof}
						We know that $\inpr a b \dc \inpr b a$ and that $X \ket \alpha \dc \bra \alpha \ha X$. Let $X \ket \alpha = \ket \gamma$ thus
						\begin{align*}
							\braket \beta X \alpha & = (\bra \beta) (\ket \gamma) \\
													& = \inpr \beta \gamma \\
													& = \inpr \gamma \beta ^* \\
													&= (\bra \alpha \ha X) (\ket \beta) ^* \\
													&= \braket \alpha {\ha X} \beta ^*
						\end{align*}
					\end{proof}
					And when $X$ is hermitian, viz. $X=\ha X$, we have $\braket \beta X \alpha = \braket \alpha X \beta ^*$
				\end{enumerate}
		\subsection {Base Kets and Matrix Representations}	
			\subsubsection{Eigenkets of an Observable}
				Let us first talk about Hermitian Operators and we will then justify the use of the word observable.\\
				\begin{theorem*}
					The eigenvalues of a Hermitian operator $A$ are real
				\end{theorem*}
				\begin{proof}
					Consider a Hermitian operator $A$. We have, following from the previous sections,
					\begin{equation}
						A\ket {a'} = a'\ket{a'}
					\end{equation}
					where $a', a'', a''', ...$ are eigenvalues for $A$, and $\ket{a'}, \ket {a''}, \ket {a'''},...$ are the corresponding eigenkets.\\
					Now since $A$ is hermitian, we know
					\begin{align*}
						A\ket{a'} &\dc \bra{a'}A \\
						\Rightarrow a'\ket{a'} &\dc a'^*\bra{a'} = \bra{a'}A
					\end{align*}
					So in general, we also have
					\begin{equation}
						\bra {a''} A = a''^* \bra{a''}
					\end{equation}
					Now we simply multiply the first relation with $\ket{a''}$ from the left and the other with $\ket{a'}$ from the right to obtain
					\begin{align}
						\braket {a''}A{a'} &= a'\inpr{a''}{a'} \\
						\braket {a''} A {a'} &= a''^* \inpr{a''}{a'}
					\end{align}
					On subtraction we get 
					\begin{equation}
						(a' - a''^*)\inpr{a''}{a'} = 0
					\end{equation}
					We are almost there. Now consider the case when $a' \ne a''$, so that $a'-a''^* \ne 0$ in general. Then for the LHS to be zero, we must have $\inpr {a''}{a'}=0$. So this proves that all eigenkets of $A$ are mutually orthogonal. Next, if $a' = a''$, then since $\inpr {a'}{a'} \ge 0$, and the equality holds only if $\ket {a'}$ is a null ket, which it is not, therefore we must have $a'-a'^* = 0 \Rightarrow a'$ is real.
				\end{proof}
				It is conventional to normalize the eigenkets to make them into an \emph{orthonormal} set as 
				\begin{equation}
					\inpr {a''} {a'} = \delta_{a'',a'}
				\end{equation}
				where $\delta_{a'',a'}$ represents the Kronecker Delta function.\\
				Further, from our assumption, the eigenkets span the eigenspace for a given operator $A$.\\
			\subsubsection{Eigenkets as Base Kets}
				Since the entire ket space can be represented by the eigenkets of $A$, we thus have
				\begin{equation}
					\ket \alpha = \sum_{a'} c_{a'}\ket{a'}
				\end{equation}
				To find the co-efficient $c_{a''}$, we just left multiply, both sides of the equation with $\bra {a''}$ to get
				\begin{equation}
					\inpr {a''}{\alpha} = c_{a''}
				\end{equation}
				(we've used the orthonormality of the eigenkets here)\\
				We thus also have
				\begin{align}
					\ket \alpha &= \sum_{a'} \ket {a'} \inpr {a'}{\alpha} \\
								&= \sum_{a'} (\ket {a'} \bra {a'}) \ket{\alpha} \\					
					\Rightarrow \sum_{a'} {\ket {a'} \bra{a'}} &= 1 &\text{(as $\ket \alpha$ is arbitrary)} \label{1_3_closure}
				\end{align}
				Equation \ref{1_3_closure} is known as the \emph{completeness relation} or \emph{closure}.\\
				Consider the following application of the completeness relation;
				\begin{align}
					\inpr \alpha \alpha &= \ket \alpha \left( \sum_{a'} \bra {a'} \ket {a'} \right) \bra \alpha \\
					 					&= \sum_{a'} (\inpr{a'}{\alpha}^2) = \sum_{a'} c_{a'}^2\\
					 					&= 1 								&\text{(if $\ket \alpha$ is normalized)}
				\end{align}
				Which easily proves a remarkable relation, with a smell of similarity with probabilities.\\
				We now declare the outer product $\ket {a'} \bra {a'}$ as the \emph{projection operator} along the ket $\ket {a'}$ and denote it by $\Lambda_{a'}$. Equation \ref{1_3_closure} can now be expressed as
				\begin{equation}
					\sum_{a'} \Lambda_{a'} = 1
				\end{equation}
				We now justify the word `projection';
				\begin{equation}
					\Lambda_{a'} \ket \alpha = \ket {a'} \inpr{a'}{\alpha} = c_{a'}\ket {a'}
				\end{equation}
			\subsubsection{Matrix Representations}
				We can write the operator $X$ as 
				\begin{equation}
					X=\sum_{a''} \sum_{a'} \ket {a''} \bra {a''} X \ket {a'} \bra {a'}
				\end{equation}
				by invoking the closure property as described earlier.
			\begin{flushright} {\small December 15, 2012} \end{flushright}
				If there're $N$ eigenkets for the ketspace of $X$, thus there would be $N^2$ $\bra{a''} X \ket {a'}$ elements. These terms can be written explicitly in an $N\times N$ matrix, which \emph{represents} the operator.
				\begin{equation}
				X \doteq
				\left(
				\begin{array}{ccccc}
					\braket{a^{(1)}}X{a^{(1)}} & \braket{a^{(1)}}X{a^{(2)}} & \braket{a^{(1)}}X{a^{(3)}}  & .. & ..\\
					\braket{a^{(2)}}X{a^{(1)}} & \braket{a^{(2)}}X{a^{(2)}} & \braket{a^{(2)}}X{a^{(3)}}  & .. & ..\\
					\braket{a^{(3)}}X{a^{(1)}} & \braket{a^{(3)}}X{a^{(2)}} & \braket{a^{(3)}}X{a^{(3)}}  & .. & ..\\
					.. & .. & .. & . \\
					.. & .. & .. &  & . \\
				\end{array}
				\right)
				\label{operator_matrix_representation}
				\end{equation}
				We already know from Equation \ref{hermitian_adjoint_operator} then, that 
				\begin{equation}
					\braket{a''}X{a'} = \braket{a'}{\ha X}{a''}^*
				\end{equation}
				{\bf Uncertain} (not the uncertainty principle) [ Given the matrix form of an operator, we can write the matrix for it's Hermitian adjoint by taking the complex conjugated transpose of the given matrix, with $\ha X$ instead of $X$, as described by the equation. ] \\
				Interestingly enough, the definition conforms with the laws of matrix multiplication to yield the corresponding operator multiplication. Take for instance
				\begin{equation}
					Z = XY
				\end{equation}
				Each element in the matrix representation of $Z$ can then be written as
				\begin{equation}				
					\braket{a^{\text{row}}}{XY}{a^{\text{column}}}
				\end{equation}
				We can introduce using closure the following without affecting equality
				\begin{equation}
					=\sum_{a'} \braket{a^{\text{row}}}{X \oupr{a'}{a'} Y}{a^{\text{column}}}				
				\end{equation}
				Which is precisely the matrix multiplication operation.
				\par
				Let us now talk about the matrix represenation of a ket $\ket \alpha$, being acted upon by an operator like so
				\begin{equation}
					\ket \gamma = X \ket \alpha
				\end{equation}
				Now to find the co-efficients of $\ket \gamma$ with respect to the eigenkets, we simply need to multiply the equation on the left by $\bra {a'}$ to obtain
				\begin{equation}
					\inpr {a'}\gamma = \braket {a'} X  \alpha
				\end{equation}
				We again use the same trick of closure, and rewrite the equation as
				\begin{equation}
					\inpr {a'}\gamma = \sum_{a'} \braket {a'} {A \ket {a''} \bra {a''}}  \alpha
				\end{equation}
				A closer look confirms the hunch that this is a square matrix multiplied by a row matrix. If we define the following representations
				\begin{equation}
				\ket \gamma \doteq
				\left(
				\begin{array}{c}
					\inpr{a^{(1)}}\gamma \\
					\inpr{a^{(2)}}\gamma \\
					\inpr{a^{(3)}}\gamma \\
					.. \\
				\end{array}
				\right)
				\end{equation}


				\begin{equation}
				\ket \alpha \doteq
				\left(
				\begin{array}{c}
					\inpr{a^{(1)}}\alpha \\
					\inpr{a^{(2)}}\alpha \\
					\inpr{a^{(3)}}\alpha \\
					.. \\
				\end{array}
				\right)
				\label{ket_matrix_representation}
				\end{equation}
				then the operation of $X$ on $\ket \alpha$ is simply a matrix multiplication;
				\begin{equation}
					\ket \gamma = X \ket \alpha
				\end{equation}
				Similarly, we have for a bra, given
				\begin{equation}
					\bra \gamma = \bra \alpha X
				\end{equation}
				the following
				\begin{equation}
					\inpr \gamma {a'} = \sum_{a''} \inpr {\alpha}{a''} \braket {a''}X{a'}
				\end{equation}
				This looks like a row (not a column) multiplied by a square matrix, which similar to kets, inspires the following representation definitions, for it to indeed be the case
				\begin{equation}
					\bra \gamma \doteq \left( \inpr \gamma {a^{(1)}}, \inpr \gamma {a^{(2)}}, \inpr \gamma {a^{(3)}}, ... \right)
					=\left( \inpr {a^{(1)}} \gamma ^*, \inpr {a^{(2)}} \gamma ^*, \inpr {a^{(3)}} \gamma ^*, ... \right)
				\end{equation}
				and similarly
				\begin{equation}
					\bra \alpha \doteq \left( \inpr {a^{(1)}} \alpha ^*, \inpr {a^{(2)}} \alpha ^*, \inpr {a^{(3)}} \alpha ^*, ... \right)
				\end{equation}
				\par
				With that done, let's talk about the inner product $\inpr \beta \alpha$ and see if it is consistent with our representations of Bras and Kets as described here. We have
				\begin{align}
					\inpr \beta \alpha 	&= \bra \beta \ket \alpha \\
										&= \sum_{a'} \bra \beta \ket {a'} \bra {a'} \ket \alpha
				\end{align}
				which is precisely, the row representation of $\bra \beta$ times the column representation of $\ket \alpha$. Also observe if we multiply corresponding representation of $\bra \alpha$ with that of $\ket \beta$, we obtain the complex conjugate of $\inpr \beta \alpha$ as represented above; consistent with the fundamental property of the inner product.
				\par
				The last representation of the section, will be that of the outer product (an operator), which is given by an $N\times N$ matrix, as we now multiply a column matrix $N\times 1$ with a row matrix $1\times N$. We thus, quite simply have
				\begin{equation}
					\oupr \beta \alpha \doteq 
					\left( 
					\begin{array}{cccc}
						\inpr{a^{(1)}}\beta^* \inpr{a^{(1)}}\alpha &\inpr{a^{(2)}}\beta^* \inpr{a^{(1)}}\alpha &\inpr{a^{(3)}}\beta^* \inpr{a^{(1)}}\alpha & .. \\
						\inpr{a^{(1)}}\beta^* \inpr{a^{(2)}}\alpha &\inpr{a^{(2)}}\beta^* \inpr{a^{(2)}}\alpha &\inpr{a^{(3)}}\beta^* \inpr{a^{(2)}}\alpha & .. \\
						.. & .. & ..
					\end{array}
					\right)
				\end{equation}
				(Note: The star is for complex conjugation, not multiplication!)\\
				If the operator is observable, say $A$, and we try to represent it using it's own eigenkets, then we get
				\begin{equation}
					A = \sum_{a'}\sum_{a''} \oupr{a''}{a''} A \oupr{a'}{a'} 
				\end{equation}
				In here, observe that $\ket {a''}$ can be thought of as representing a column matrix of $1 \times N$, and $\bra {a'}$ as a row matrix of $N \times 1$, and therefore $\braket {a''}A{a'}$ should be represented by an $N \times N$ square matrix. Since $\ket {a'}$ and $\ket {a''}$ are eigenkets, thus we have
				\begin{align}
					\bra {a''} A \ket {a'} 	&= \bra {a''} a' \ket {a'}\\
											&= \bra {a''} \ket{a'} a' \\
											&= \inpr{a'}{a'} a' \delta_{a',a''} \\
											&= a' \delta_{a',a''}
				\end{align}
				which basically shows the matrix is diagonal. Now going back, we have
				\begin{align}
					A 	&= \sum_{a'}\sum_{a''} \ket{a''} a' \delta_{a',a''} \bra{a'}
						= \sum_{a'}\sum_{a''} \ket{a'} a' \delta_{a',a''} \bra{a'} \\
						&= \sum_{a'}\sum_{a''} a' \ket{a'} \bra{a'}  \delta_{a',a''}
						= \sum_{a'}\sum_{a''} a' \Lambda_{a'}  \delta_{a',a''}\\
						&= \sum_{a'} a' \Lambda_{a'} \label{observable_operators_expansion}
				\end{align}
				Note here, that the matrix representing the operator $A$ is both diagonal and real. Which implies that it's complex conjugated transpose will be identical to itself; which is consistent with $A$ being Hermitian.				
			\subsubsection{Spin $\frac 1 2$ Systems}
				\begin{flushright} December 17, 2012 \end{flushright}
				Let the two eigenkets be $\ket +$ and $\ket -$. Then from the definition of the identity operator, we have
				\begin{equation}
					1 = (\oupr + + ) + (\oupr - -)					
				\end{equation}
				Now we can define the $S_z$ operator as
				\begin{equation}
					A = \frac \hbar 2 \left[ (\oupr + + ) -  (\oupr --) \right]
					\label{operator_twostate}
				\end{equation}
				using equation \ref{observable_operators_expansion} and the eigenvalues for $\ket +$ and $\ket -$. We can easily find the eigenvalues using this expression for $A$, by simply multiplying the corresponding eigenket and invoking their orthonormality to get
				\begin{equation}
					A \ket \pm = \pm \frac \hbar 2 \ket \pm
				\end{equation}
				Before we try to represent them as matrices, let us first look at two more operators of interest in Quantum Mechanics (as we shall hopefully see later)
				\begin{equation}
					S_+ = \hbar \oupr + -
				\end{equation}
				This operator can be thought of as one that increases spin. If we operate this on $\ket -$ we get $\frac \hbar 2 \ket +$ and if we operate on $\ket +$, we get a Null ket; consistent with the idea, as $\ket +$ is already the highest spin in this system. Similarly we also have 
				\begin{equation}
					S_- = \hbar \oupr - +
				\end{equation}
				Now we come to the matrix representations. It is customary (or so I've been told) to write the highest angular momentum component first, in the row indices. So we therefore have from equation \ref{ket_matrix_representation},
				\begin{align}
				\ket + \doteq
				\left(
				\begin{array}{c}
					1 \\
					0
				\end{array}
				\right)
				& &\ket - \doteq
				\left(
				\begin{array}{c}
					0 \\
					1
				\end{array}
				\right)
				\end{align}
				Using equation \ref{operator_matrix_representation}, we can write the matrices for all three operators discussed here;
				\begin{align}
				S_z \doteq
				\frac \hbar 2
				\left(
				\begin{array}{cc}
					1 &0 \\
					0 &-1
				\end{array}
				\right)
				& &S_+ \doteq
				\hbar
				\left(
				\begin{array}{cc}
					0 &1 \\
					0 &0
				\end{array}
				\right)
				& &S_- \doteq
				\hbar
				\left(
				\begin{array}{cc}
					0 &0 \\
					1 &0
				\end{array}
				\right)
				\end{align}
				Observe that neither $S_+$ nor $S_-$ are hermitian (since their complex conjugated transpose aren't identical to themselves)\\
				{\bf Doubt} Why is there $\hbar$ instead of $\displaystyle \frac \hbar 2$ in the spin increase/decrease operators?
		\subsection{Measurements, Observables and the Uncertainty Relations}
			\subsubsection{Measurements}
				In the words of PAM Dirac ``A measurement always causes the system to jump into an eigenstate of the dynamical variable that is being measured'' Details have been omitted from the discussion, as we'd covered them in our prior course.\\
				Postulate: 
				\begin{equation}
					\text{Probability of jumping into a particular state given by } \ket {a'} = \inpr {a'} \alpha ^2
				\end{equation}
				granted $\ket \alpha$ is normalized. \\
				Definition: \emph{Expectation Value} of $A$ with respect to a state $\ket \alpha$ is given by
				\begin{equation}
					 \expval A \equiv \braket \alpha A \alpha
				\end{equation}
				We can use the closure property and obtain
				\begin{align}
					\expval A 	&= \sum_{a'}\sum_{a''} \inpr \alpha {a''} \braket {a''} A {a'} \inpr {a'} \alpha \\
								&= \sum_{a'} a' \inpr {a'} \alpha ^2
				\end{align}
				We have used orthonormality of eigenkets, the fact that they are eigenkets, and that the inner products are complex numbers. This shows that the expectation value agrees with the notion of \emph{average measured value}.
				\par
				Note that expectation value is NOT identical to eigenvalues. For instance, for a spin $\frac 1 2$ system, for $S_z$ the eigenvalues can only be $+\hbar/2$ and $-\hbar/2$, whereas it's expectation value is a real number between $+\hbar/2$ and $-\hbar/2$.
				\par
				\emph{Pure Ensemble} is a collection of identically prepared physical systems, viz. all are characterized by the same ket, $\ket \alpha$. For instance, the $+ \hat z$ beam through an SG$\hat z$ apparatus.
				\par
				\emph{Selective Measurement} is a measurement that selects only one of the eigenkets of an observable, $A$, say $\ket a'$. Mathematically we can represent this by the projection operator on $\ket \alpha$ as
				\begin{equation}
					\Lambda_{a'}\ket \alpha = \ket {a'} \inpr{a'} \alpha
				\end{equation}
			\subsubsection{Spin $\frac 1 2$ Systems, Again}
\vspace{12pt}
\hrule

\end{document}
